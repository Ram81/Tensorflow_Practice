{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed348e31-67d7-4385-ab76-31eb4a5ffced",
    "_uuid": "f6cbd47eadccedab5566ba4502801f85644e475c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38105b59-280d-4d59-91b1-4c2a5251f778",
    "_uuid": "131b0485cbc80136db6d0167f5718d1332d43639",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "EMBEDDING_PATH = '../input/'\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18588ba9-70f1-4278-aed9-4f1c21a0818a",
    "_uuid": "1ce2fad133f07b7dde3282fe15b8059e8794b582",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Embedding\n",
    "print('load word embedding...')\n",
    "embedding_index = {}\n",
    "f = codecs.open('../input/fasttext/wiki.simple1.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "759ce5d6-dbc5-41e3-a8cd-ef8018ee4bde",
    "_uuid": "9b6b1a0f7318fd01f33899feb590bba414e1cdd5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "input_dir = '../input/jigsaw-toxic-comment-classification-challenge/'\n",
    "train_df = pd.read_csv(input_dir + 'train.csv', sep=',', header=0)\n",
    "test_df = pd.read_csv(input_dir + 'test.csv', sep=',', header=0)\n",
    "\n",
    "test_df = test_df.fillna('_NA_')\n",
    "print('Training DF: '+str(train_df.shape))\n",
    "print('Testing DF: '+str(test_df.shape))\n",
    "\n",
    "label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "# Visualize\n",
    "train_df['doc_len'] = train_df['comment_text'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "sns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc_len')\n",
    "plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
    "plt.title('comment length');\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d0207eb-54d6-4e78-a629-e6fbae4b6ef4",
    "_uuid": "8d48b9331c51a9e1a7898e5c96408b6d3f0d69fd",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_docs_train = train_df['comment_text'].tolist()\n",
    "raw_docs_test = test_df['comment_text'].tolist()\n",
    "num_classes = len(label_names)\n",
    "\n",
    "print('pre-processing training data')\n",
    "processed_train_data = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_train_data.append(\" \".join(filtered))\n",
    "\n",
    "print('pre-processing test data')\n",
    "processed_test_data = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_test_data.append(\" \".join(filtered))\n",
    "\n",
    "print(\"Tokenizing input data....\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_train_data + processed_test_data)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_train_data)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_test_data)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Dictionary Size: \"+str(len(word_index)))\n",
    "\n",
    "# Pad Sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db96b6ea-c566-4a72-96b5-05ecd5410b77",
    "_uuid": "5a2c7a4b78c3f85e2a4813c7b0c888601ba15815",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "batch_size = 256\n",
    "num_epochs = 8\n",
    "\n",
    "num_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f94e549-f7a7-4c15-bb03-6f8a6574e5cb",
    "_uuid": "61ca02a1d696bb9490e2f627b76a1443e301655e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if(i>=nb_words):\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if(embedding_vector is not None) and len(embedding_vector)>0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('Number of null words embeddings found %d' % np.sum(np.sum(embedding_matrix, axis=1)==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ad9377b0-13a4-410b-90a7-e56a533655da",
    "_uuid": "4be9e1f5ade7ed2b05a57a53477bd017b9138a3c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(' Sample words not found: ',np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10f28c23-c23f-4bce-8aad-d52087cf151e",
    "_uuid": "019e731862f07750276313321ddd3a0269861520",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Building CNN')\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a52d6746-a594-43ce-a1e6-2da394378e00",
    "_uuid": "0fdd54ca8eb912169c3816b074ef572ff59592b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callback_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b7704c9-d95b-446b-b8df-310b3dc6ca4f",
    "_uuid": "53897328805acb3277e9e2ea8a613222300ccab2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training CNN\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callback_list, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f840345e-28f0-4976-94a2-c875482f1609",
    "_uuid": "330dbbd5292cb0f77cda43f3a40c2051e01eac32",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "57b10854-4e04-44cb-a7c2-9880e0451eaa",
    "_uuid": "1de72c194c733c2588c3b33d35a0dc2df40cfc0a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a submission\n",
    "submission_df = pd.DataFrame(columns=['id'] + label_names)\n",
    "submission_df['id'] = test_df['id'].values \n",
    "submission_df[label_names] = y_test \n",
    "submission_df.to_csv(\"./submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4bb57419-ef30-46fd-a8c8-e06e1195e377",
    "_uuid": "b756ff2e1edf0c44b867a34218bd79bf0e34b4ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN Toxic Sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Binary Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97940a79-621b-407b-9f7c-c1cf2566da0a",
    "_uuid": "9fbee0a2f73fbe942745e5a42a5324c62cde1b43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN Toxic Sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
